// -----------
// usage:
// webppl aidabaseline.wppl --require webppl-json


// core data structure: cluster.


//------------------------------------------------
// Data generated by generate_wppl.py
//------------------------------------------------

// entries:
// theGraph: characterization of statement and ERE nodes in the graph.
//           mapping from node labels to node structures
// statementProximity: pairwise proximity between statement nodes
//           sparse, hence encoded as a mapping from statement node labels
//           to statement node labels to proximities
// entryPoints: list of entryPoint structures;
//           they are alternative cluster seeds to start from
//           an entry point is a structure that describes a cluster,
//           the core data structure described above
var aidaData = json.read('aidagraph.json');

//------------------------------------------------
// General functions
//------------------------------------------------

// remove duplicates in an array
var myUnique = function(arr) {
    return reduce(function(index, acc) {
	return arr.indexOf( arr[ index ] ) < index ? acc : [ arr[index ]].concat(acc)
    }, [ ],  _.range(arr.length))
}

// add a second array to a first one while omitting duplicates in the 2nd array
var myAddUnique = function(arr1, arr2) {
    return reduce( function(entry, acc) {
	return arr1.indexOf(entry) > -1 ? acc : [ entry ].concat(acc)
    }, arr1, arr2)
}

// return all members in array1 that are not in array2
var mySetDiff = function(arr1, arr2) {
    return filter(
	function(entry) {
	    return arr2.indexOf(entry) == -1
	}, arr1)
}

// intersection of two arrays
var myIntersection = function(array1, array2) {
    return filter(
	function(e) {
	    return array2.indexOf(e) > -1
	},
	array1)
}
	

// return array minus all elements that are equal to elt
var myArrayMinusElt = function(somearray, elt) {
    return filter(function(e) { return e != elt }, somearray)
}



//------------------------------------------------
// Coreference handling via unification
//------------------------------------------------


// seqOfEres a list of all entities and events (no relations, they do not appear in clusters), along with variables that appear in entry points.
// all the unification functions below use the order of EREs and variables in this list
// as a basis for the unifiers they compute.
// seqConstraintVar is a sequence of core constraint variables from all entrypoints, without duplicates
var seqConstraintVar = filter(
    function(entry) {
	// retain only variables, i.e. stuff that doesn't have an (ERE) entry in the graph
	return !(_.has(aidaData.theGraph, entry))
    }, myUnique(
	// remove duplicates in
	// flattened arguments of query constraints
	reduce(
	    function(entryPoint, acc1) {
		return reduce(
		    function(cconstraint, acc2) {
			return [ cconstraint[0], cconstraint[2]].concat(acc2)
		    }, acc1, entryPoint.queryConstraints)
	    }, [ ], aidaData.entrypoints)
    )
)

var seqOfEres = filter(
    function(k) {
	return aidaData.theGraph[k].type == "Event" ||
	    aidaData.theGraph[k].type == "Entity"
    }, _.keys(aidaData.theGraph)).concat(seqConstraintVar)

// returns the unifier of an ERE or constraint variable
var unifierGet = function(ere, unifInfo) {
    return unifInfo[ seqOfEres.indexOf(ere) ]
}

// true if two EREs or constraint variables have the same unifier
var unifierEquals = function(ere1, ere2, unifInfo) {
    return unifierGet(ere1, unifInfo) == unifierGet(ere2, unifInfo)
}

// unifiable: either they are already in the same equivalence class,
// or one of them is an unbound core variable.
var unifierUnifiable = function(ere1, ere2, unifInfo) {
    return unifierVarUnbound(ere1, unifInfo) ||
	unifierVarUnbound(ere2, unifInfo) ||
	unifierEquals(ere1, ere2, unifInfo)
}

// given two unifiers, decide which becomes the unifier of both and which becomes obsolete.
// returns structure with entries newUnifier, obsoleteUnifier.
// see to it that query variables don't become unifiers.
var unifierDecideUnifier = function(unifier1, unifier2) {
    if (unifier1[0] == "?") {
	return {
	    newUnifier : unifier2,
	    obsoleteUnifier : unifier1
	}
    } else {
	if (unifier2[0] == "?") {
	    return {
		newUnifier : unifier1,
		obsoleteUnifier : unifier2
	    }
	} else {
	    return {
		newUnifier : sort([unifier1, unifier2])[0],
		obsoleteUnifier : sort([unifier1, unifier2])[1]
	    }
	}
    }
}	
	    

// returns a new unifInfo in which ere1 and ere2 have been unified
var unifierUnify = function(ere1, ere2, unifInfo) {
    var unifier1 = unifierGet(ere1, unifInfo)
    var unifier2 = unifierGet(ere2, unifInfo)

    if (unifier1 == unifier2) {
	return unifInfo
    } else {
	var unifierOrder = unifierDecideUnifier(unifier1, unifier2)
	return map(
	    function(unifier) {
		return unifier == unifierOrder.obsoleteUnifier ? unifierOrder.newUnifier : unifier
	    }, unifInfo)
    }
}

// make an initial unifier that maps each ERE or variable to itself.
var unifierInitial = function() {
    return seqOfEres
}

// unbound core variable:
// true if coreVar doesn't have an entry in theGraph (so it is a core variable)
// and its unifier is itself
var unifierVarUnbound = function(coreVar, unifInfo) {
    return !(_.has(aidaData.theGraph, coreVar)) && unifierGet(coreVar, unifInfo) == coreVar
}


//------------------------------------------------
// Access to the data generated by generate_wppl.py
//------------------------------------------------

// given a graph label, find the matching entry
var aidaEntry = function(label) {
    return aidaData.theGraph[ label ]
}

// read the proximity between two statements off the AIDA graph
var aidaProximity = function(stmt1, stmt2) {
    if (_.has(aidaData.statementProximity, stmt1) && _.has(aidaData.statementProximity[stmt1], stmt2)) {
	// we have an entry for stmt1, and a proximity to stmt2: return that
	return aidaData.statementProximity[stmt1][stmt2]
    } else {
	return 0.0
    }
}


// take a list of statements and map them to a list of statement entries
var aidaStatementEntries = function(stmtList) {
    return map(
	function(stmt) {
	    return aidaEntry(stmt)
	},
	stmtList)
}


// find all EREs that are mentioned in a given statement
var aidaEreInStatement = function(statement) {
    // retrieve the entry 
    var statementEntry = aidaEntry(statement)
    
    // return subject and objects for this entry
    // that again have an entry in "theGraph" and have an ERE type
    return myUnique(
	filter(
	    function(entry) {
		return _.has(aidaData.theGraph, entry) &&
		    (aidaData.theGraph[entry].type == "Entity" ||
		     aidaData.theGraph[entry].type == "Event" ||
		     aidaData.theGraph[entry].type == "Relation")
	    },
	    map(
		function(role) {
		    return statementEntry[role] },
		["subject", "object"]
	    )
	)
    )
}


// given a list of ERE names and a list of statements,
// find the statements adjacent to each ERE,
// and form the union of all those with the old statements
var aidaAddStatementsFromEre = function(oldStatements, EreList) {
    return reduce(
	function(ere, acc1) {
	    return reduce(
		function(stmt, acc2) {
		    return acc2.indexOf(stmt) > -1 ? acc2 : acc2.concat( stmt )
		},
		acc1, aidaData.theGraph[ere].adjacent)    
	},
	oldStatements, EreList)
}

// get statement triple: given statement index,
// find the statement in theGraph that matches it,
// and transform {subject: A, predicate: B, object:C } to [ A, B, C]
var aidaStatementTriple = function(stmt) {
    var stmtEntry = aidaEntry(stmt)

    return [ stmtEntry.subject, stmtEntry.predicate, stmtEntry.object ]
}

// return all the statements  from cluster.candidates that fill the
// given query constraint.
var aidaStatementMatchingConstraint = function(queryConstraint, cluster) {    
    return filter(
	function(stmt) {
	    var stmtTriple = aidaStatementTriple(stmt)

	    // match if the predicates are the same,
	    // and the arguments are unifiable, i.e.:
	    // either in the same equivalence class, or an unfilled variable and an ERE
	    return queryConstraint[1] == stmtTriple[1] &&
		unifierUnifiable(queryConstraint[0], stmtTriple[0], cluster.coref.unifier) &&
		unifierUnifiable(queryConstraint[2], stmtTriple[2], cluster.coref.unifier)
	    
	}, cluster.candidates)
   
}


//-------------------------------
// Proximity measures
//-----

// candidate proximity: how close is the candidate to the cluster?
// cluster: structure as above
// candidate: statement label
var candidateProximity = function(cluster, candidate) {
    return averageProximity(cluster, candidate)
}


// candidate proximity as proximity to a random cluster member
var randomMemberProximity = function(cluster, candidate) {
    var randomMember = uniformDraw(cluster.cluster.statements)
    return aidaProximity(randomMember, candidate)
}

// candidate proximity as average proximity:
// 1/clustersize sum_{cluster member i} proximity(i, candidate)
// when proximities are random walk probabilities, this is the probability
// of ending up at candidate when taking a one-step random walk from any cluster member.
var averageProximity = function(cluster, candidate) {
    return sum(
	map(
	    function(stmt) {
		return aidaProximity(stmt, candidate)
	    }, cluster.cluster.statements))  / cluster.cluster.statements.length
}



//-------------------------------
// Logical consistency
//-----

var objectUniquePredicatesHard = ["hasKBEntry", "type"];
var objectUniquePredicates = [ 'Life.Die_Agent', 'Movement.TransportPerson_Person',
			       'Manufacture.Artifact_Manufacturer', 'Movement.TransportPerson_Origin',
			       'Life.Die_Victim', 'Manufacture.Artifact_Artifact', 'Conflict.Attack_Attacker',
			       'Justice.Convict_Defendant', 'Transaction.TransferOwnership_Thing', 'GeneralAffiliation.APORA_Affiliation',
			       'Life.Die_Instrument', 'Conflict.Attack_Instrument', 'Justice.Appeal_Prosecutor',
			       'Movement.TransportArtifact_Destination', 'Contact.Broadcast_Broadcaster',
			       'Conflict.Attack_Target', 'GeneralAffiliation.Sponsorship_Sponsor', 'Movement.TransportArtifact_Origin',
			       'Justice.Appeal_Defendant', 'Justice.Convict_Adjudicator', 'Movement.TransportArtifact_Artifact',
			       'Transaction.TransferOwnership_Recipient', 'Movement.TransportArtifact_Agent',
			       'OrganizationAffiliation.EmploymentMembership_Organization', 'Transaction.TransferOwnership_Giver',
			       'Transaction.TransferOwnership_Beneficiary', 'GeneralAffiliation.Sponsorship_Entity',
			       'Movement.TransportArtifact_Instrument', 'Movement.TransportPerson_Agent', 'Movement.TransportPerson_Instrument',
			       'GeneralAffiliation.MORE_Affiliation' ];

// object uniqueness with hard constraint:
// if p is an object unique predicate, then
// if e1 p e2 and e1 p e3 then e2 = e3
var constObjectUniqueHard = function(clusterTriples, candidateTriple, cluster) {
    // test for subject uniqueness only if our candidate has a predicate that is subject-unique
    if (objectUniquePredicatesHard.indexOf(candidateTriple[1]) > -1) {
	map(
	    function(clusterTriple) {
		condition(!(unifierEquals(candidateTriple[0], clusterTriple[0], cluster.coref.unifier)) ||
			  candidateTriple[1] != clusterTriple[1] ||
			  unifierEquals(candidateTriple[2], clusterTriple[2], cluster.coref.unifier))
	    }, clusterTriples)
    }
}

// object uniqueness, soft constraint:
// if p is an object unique predicate, then
// if e1 p e2 and e1 p e3 then e2 = e3
// This is a soft constraint
var constObjectUnique = function(clusterTriples, candidateTriple, cluster) {
    // test for subject uniqueness only if our candidate has a predicate that is subject-unique
    if (objectUniquePredicates.indexOf(candidateTriple[1]) > -1) {
	map(
	    function(clusterTriple) {
		factor(!(unifierEquals(candidateTriple[0], clusterTriple[0], cluster.coref.unifier)) ||
		       candidateTriple[1] != clusterTriple[1] ||
		       unifierEquals(candidateTriple[2], clusterTriple[2], cluster.coref.unifier) ?
		       0 : -3)
	    }, clusterTriples)
    }
}

// do use type statements
var constUseTypes = function(cluster, candidateEntry) {
    factor(candidateEntry.predicate == "type" ? 0.1 : 0)
}

// take confidence into account
var constConfidence = function(cluster, candidateEntry) {
    factor(Math.log(candidateEntry.conf))
}

// a list of all the constraints to apply to pairs of statement triples.
var pairwiseConstraints = [ constObjectUniqueHard, constObjectUnique ];
// a list of all constraints to apply to the candidate statement
var unaryConstraints = [ constUseTypes, constConfidence ];

// test the logical consistency of the given cluster if candidate was to be added to it
// cluster: structure.
// candidate: statement index
var logicalConsistency = function(cluster, candidate) {
    // map statements to triples
    var candidateTriple = aidaStatementTriple(candidate)
    var clusterTriples = map(
	function(i) {
	    return aidaStatementTriple(i)
	}, cluster.cluster.statements)

    // apply binary constraints on triples
    map(
	function(constraintFn) {
	    constraintFn(clusterTriples, candidateTriple, cluster)
	}, pairwiseConstraints)

    // and apply unary constraints on the candidat entry
    var candidateEntry = aidaEntry(candidate)
    map(
	function(constraintFn) {
	    constraintFn(cluster, candidateEntry)
	}, unaryConstraints)
}

//-------------------------------
// Functions that check and manipulate cluster objects
//-----

var clusterObjectInit = function(clusterSeed, corefFn) {
    // make a cluster of our internal format
    return {
	// cluster members
	cluster : {
	    ere : clusterSeed.ere,
	    statements : clusterSeed.statements,
	    corefStatements : [ ],
	    corefGroups : [ ]
	},
	// queries to be fulfilled
	query : {
	    constraints : clusterSeed.queryConstraints,
	    failed : [ ]
	},
	// to handle coreference, we carry around a unifier object
	// that records which EREs corefer, as well as a function
	// that shows which coref statements are true
 	coref: {
	    unifier : unifierInitial(),
	    corefGroupMember : corefFn
	},
	// statements adjacent to EREs in the cluster for which we have not decided
	// yet whether they should go in
	candidates : [ ],
	// statements for which we have already decided whether they go in the cluster or not
	considered : clusterSeed.statements
    }
}

var clusterObjectUpdateCluster = function(cluster, ere, statements, corefStatements, corefGroups) {
    return {
	cluster : {
	    ere : ere,
	    statements : statements,
	    corefStatements : corefStatements,
	    corefGroups : corefGroups
	},
	query : cluster.query,
	coref : cluster.coref,
	candidates : cluster.candidates,
	considered : cluster.considered
    }
}

var clusterObjectUpdateQuery = function(cluster, queryConstraints, failedConstraints) {
    return {
	cluster : cluster.cluster,
	query : {
	    constraints : queryConstraints,
	    failed : failedConstraints
	},
	coref : cluster.coref,
	candidates : cluster.candidates,
	considered : cluster.considered
    }	
}

var clusterObjectUpdateCoref = function(cluster, unifier, corefFn) {
    return {
	cluster : cluster.cluster,
	query : cluster.query,
	coref : {
	    unifier : unifier,
	    corefGroupMember : corefFn
	},
	candidates : cluster.candidates,
	considered : cluster.considered
    }
}

var clusterObjectUpdateCandidates = function(cluster, candidates) {
    return {
	cluster : cluster.cluster,
	query : cluster.query,
	coref : cluster.coref,
	candidates : candidates,
	considered : cluster.considered
    }
}

var clusterObjectUpdateConsidered = function(cluster, considered) {
    return {
	cluster : cluster.cluster,
	query : cluster.query,
	coref : cluster.coref,
	candidates : cluster.candidates,
	considered : considered
    }
}

var clusterNoMoreQueryConstraints = function(cluster) {
    return cluster.query.constraints.length == 0
}

var clusterNoMoreCandidateStatements = function(cluster) {
    return cluster.candidates.length == 0
}

// compute new cluster when the candidate is added
var clusterAddCandidate = function(cluster, candidate) {
    
    // compute updated cluster components
    var newEre = mySetDiff( aidaEreInStatement(candidate), cluster.cluster.ere)
    var updatedStatements = cluster.cluster.statements.concat(candidate)
    var updatedConsidered = cluster.considered.concat(candidate)
    var updatedCandidates = myArrayMinusElt(cluster.candidates, candidate)
    
    // candidates: will be done later.

    var updated1 = clusterObjectUpdateCluster(cluster,
					      cluster.cluster.ere.concat( newEre ),
					      updatedStatements,
					      cluster.cluster.corefStatements,
					      cluster.cluster.corefGroups)
    var updated2 = clusterObjectUpdateConsidered(updated1, updatedConsidered)
    var updated3 = clusterObjectUpdateCandidates(updated2, updatedCandidates)

    // display(["updated: addCandidate yields", updated3.cluster.ere, updated3.cluster.statements])
    
    return {
	newCluster : updated3, 
	newERE : newEre,
	newStatements : [ candidate ]
    }
}

// compute new cluster when the candidate is not added
var clusterDropCandidate = function(cluster, candidate) {
    var updatedCandidates = myArrayMinusElt(cluster.candidates, candidate)
    var updatedConsidered = cluster.considered.concat(candidate)

    // display("not adding")
    // display(["cand", updatedCandidates])
    // display(["stmt CAP cand", myIntersection(cluster.statements, updatedCandidates)])

    var updated1 = clusterObjectUpdateCandidates(cluster, updatedCandidates)
    var updated2 = clusterObjectUpdateConsidered(updated1, updatedConsidered)

    // display(["update dropCandidate yields", updated2.candidates])
    
    return {
	newCluster : updated2, 
	newERE : [ ],
	newStatements : [ ]
    }
}

// core extension: nothing to do. just remove the candidate from the core constraints
var clusterQueryNothingToDo = function(cluster, queryConstraint) {
    var updatedQueryConstraints = myArrayMinusElt(cluster.query.constraints, queryConstraint)
    
    return {
	newCluster : clusterObjectUpdateQuery(cluster, updatedQueryConstraints, cluster.query.failed),
	newERE : [ ],
	newStatements : [ ]
    }
}

// core extension: could not fulfil query. List it as failed.
var clusterQueryFailed = function(cluster, queryConstraint) {
    var updatedQueryConstraints = myArrayMinusElt(cluster.query.constraints, queryConstraint)
    
    return {
	newCluster : clusterObjectUpdateQuery(cluster, updatedQueryConstraints, cluster.query.failed.concat(queryConstraint)),
	newERE : [ ],
	newStatements : [ ]
    }
}



// found a candidate to fill a query constraint. add it
var clusterQueryAdd = function(cluster, queryConstraint, candidate) {
    
    // omit the query constraint we just fulfilled
    var updatedQueryConstraints = myArrayMinusElt(cluster.query.constraints, queryConstraint)

    // unify the first argument of the candidate with the first variable in the constraint,
    // and the second argument of the candidate with the second variable in the constraint
    var candidateTriple = aidaStatementTriple(candidate)
    var updatedUnifier1 = unifierUnify(candidateTriple[0], queryConstraint[0], cluster.coref.unifier)
    var updatedUnifier = unifierUnify(candidateTriple[2], queryConstraint[2], updatedUnifier1)

    var updated1 = clusterObjectUpdateQuery(cluster, updatedQueryConstraints, cluster.query.failed)
    var updated2 = clusterObjectUpdateCoref(updated1, updatedUnifier, cluster.coref.corefGroupMember)

    // display(["updated queryAdd yields", updated2.query])
    
    // the rest is as in clusterAddCandidate, so use that
    return clusterAddCandidate( updated2, candidate)
}


//-------------------------------
// Handling coreference
//-----

// given a list of EREs for which we still need to determine coreference:
// determine their adjacent coref groups, sample whether they are a member.
// If they are member of a coref group, sample membership of all that coref group's
// potential member EREs.
// For all EREs for which we sample that they are members, again determine their
// adjacent coref groups and so on.
var corefDo = function(cluster, ereList) {
    // returns a structure containing: 
    // newERE, newCluster.
    return corefSample(cluster, ereList, [ ])
}

// throw a coin to see whether this coref statement should be a new member of the cluster
var corefIsNewMember = function(cluster, corefStmt) {
    var corefGroup = aidaData.theGraph[corefStmt].cluster
    var member = aidaData.theGraph[corefStmt].clusterMember
    var confidence = aidaData.theGraph[corefStmt].conf

    // coref group not in the cluster yet, and we've sampled that this ere is in the group
    return (cluster.cluster.corefGroups.indexOf(corefGroup) == -1 && cluster.coref.corefGroupMember(corefGroup, member, confidence))   
}

var corefSample = function(cluster, todoEreList, doneEreList) {
    if (todoEreList.length == 0) {
	// display("done with coref")
	return {
	    newCluster : cluster,
	    newEre : doneEreList
	}
    } else {

	// display(["coref starting with ERE", todoEreList])
	
	// which coref statements are we adding?
	// the ones for which coref groups are not in the cluster yet, and we've sampled
	// that this ere is in the group
	var newCorefStmtsFromEre = reduce(
	    function(ere, acc) {
		return (_.has(aidaData.theGraph, ere) & _.has(aidaData.theGraph[ere], "adjacentCoref")) ?
		    acc.concat(filter(
			function(corefStmt) {
			    return corefIsNewMember(cluster, corefStmt)
			}, aidaData.theGraph[ere].adjacentCoref))
		:
		acc
	    }, [ ], todoEreList
	)

	// display(["adding coref statements from EREs:", newCorefStmtsFromEre])

	// new coref groups
	var newCorefGroups = myUnique(map(
	    function(corefStmt) {
		return aidaData.theGraph[corefStmt].cluster
	    }, newCorefStmtsFromEre
	))

	// display(["new coref groups", newCorefGroups])

	// additional coref statements from these new groups
	var newCorefStmtsFromGroup = reduce(
	    function(corefGroup, acc) {
		return acc.concat(filter(
		    function(corefStmt) {
			return corefIsNewMember(cluster, corefStmt)
		    }, aidaData.theGraph[corefGroup].adjacentMembers))
	    }, [ ], newCorefGroups)

	// display(["new coref stmts from groups", newCorefStmtsFromGroup])
	
	// new ERE: coref members from the new coref statements, but not in the cluster yet
	var newEre = filter(
	    function(ere) {
		return cluster.cluster.ere.indexOf(ere) == -1
	    }, map(
		function(corefStmt) {
		    return aidaData.theGraph[corefStmt].clusterMember
		}, newCorefStmtsFromGroup)
	)

	// display(["new ERE", newEre])

	// make new cluster with all this info:
	// add both sets of coref statements, set of new coref groups, new EREs
	
	var cluster1 = clusterObjectUpdateCluster(cluster,
						  myUnique(cluster.cluster.ere.concat(newEre)),
						  cluster.cluster.statements,
						  myUnique(cluster.cluster.corefStatements.concat(newCorefStmtsFromEre).concat(newCorefStmtsFromGroup)),
						  myUnique(cluster.cluster.corefGroups.concat(newCorefGroups)))

	// display(["cluster now corefStmt", cluster1.cluster.corefStatements, "corefGroups", cluster1.cluster.corefGroups])
	
	// do unification based on all coref statements
	var cluster2 = reduce(
	    function(corefStmt, clAcc) {
		var clMember = aidaData.theGraph[corefStmt].clusterMember
		var corefGroup = aidaData.theGraph[corefStmt].cluster
		var clPrototype = aidaData.theGraph[ corefGroup ].prototype

		// display(["now unifying", clMember, clPrototype])

		return clusterObjectUpdateCoref(clAcc, unifierUnify(clMember, clPrototype, clAcc.coref.unifier), clAcc.coref.corefGroupMember)
	    }, cluster1, myUnique(newCorefStmtsFromEre.concat(newCorefStmtsFromGroup)))

	// display(["changed unifiers",
	// 	 map(
	// 	     function(ix) {
	// 		 return [seqOfEres[ix], cluster1.coref.unifier[ix], cluster2.coref.unifier[ix]]
	// 	     }, filter(
	// 		 function(ix) {
	// 		     return cluster2.coref.unifier[ix] != cluster1.coref.unifier[ix]
	// 		 }, _.range(cluster2.coref.unifier.length)))
	// 	])

	return corefSample(cluster2, newEre, doneEreList.concat(todoEreList))
    }
}


//-------------------------------
// Extending the cluster
//-----

// extend the cluster once, either by choosing a core constraint to fill, if there is one,
// or by choosing a statement that we haven't considered yet that is adjacent to one
// of the cluster's EREs
var extendClusterOnce = function(cluster, thresholdParameters) {
    // if there are no query constraints, add a candidate statement from the frontier.
    if (clusterNoMoreQueryConstraints(cluster)) {
	return extendClusterOnceByCandidateStatement(cluster, thresholdParameters)
    } else {
	// we do still have core constraints to do
	return extendClusterOnceByQueryConstraint(cluster, thresholdParameters)
    }
}

// extend the cluster once by choosing a core constraint to fill
// this returns a structure with entries
// newCluster: updated cluster
// newERE: EREs added to the frontier of the cluster
// newStatements: statements added to the cluster
var extendClusterOnceByQueryConstraint = function(cluster, thresholdParameters) {
    if (clusterNoMoreQueryConstraints(cluster)) {
	// nothing to do after all
	return {
	    newCluster : cluster,
	    newERE : [ ],
	    newStatements : [ ]
	}
    } else {
	// query constraints remaining. randomly choose one of them.
	var queryConstraint = uniformDraw(cluster.query.constraints)
	// display(["considering query constraint", queryConstraint])

	// constraint has the form ?entry1 Rel ?entry2
	var arg1 = queryConstraint[0]
	var arg2 = queryConstraint[2]

	if (!unifierVarUnbound(arg1, cluster.coref.unifier) && !unifierVarUnbound(arg2, cluster.coref.unifier)) {
	    // nothing to do, this query constraint is already fulfilled
	    // display("queryConstraint done")
	    return clusterQueryNothingToDo(cluster, queryConstraint)
	} else {
	    if (unifierVarUnbound(arg1, cluster.coref.unifier) && unifierVarUnbound(arg2, cluster.coref.unifier)) {
		// we cannot possibly fill this constraint at this point.
		// don't fill it, and give this sample a penalty
		// display(["queryConstraint cannot be filled", queryConstraint])
		factor(-5)
		return clusterQueryFailed(cluster, queryConstraint)
	    } else {
		// draw a random statement that can fill this query facet.
		var candidates = aidaStatementMatchingConstraint(queryConstraint, cluster)
		// display(["queryConstraint candidates", candidates])
		if (candidates.length == 0) {
		    // no match for this constraint. just don't fill it then,
		    // and give this sample a penality
		    // display(["queryConstraint cannot be filled", queryConstraint])
		    factor(-5)
		    return clusterQueryFailed(cluster, queryConstraint)
		} else {
		    var candidate = uniformDraw(candidates)
		    // display(["filling query constraint with candidate", candidate])
		    return clusterQueryAdd(cluster, queryConstraint, candidate)
		}
	    }
	}
    }
}

// extend the cluster once by choosing a statement from the candidate list
// this returns a structure with entries
// newCluster: updated cluster
// newERE: EREs added to the frontier of the cluster
// newStatements: statements added to the cluster
var extendClusterOnceByCandidateStatement = function(cluster, thresholdParameters) {
    if (clusterNoMoreCandidateStatements(cluster)) {
	// nothing to do after all
	return {
	    newCluster : cluster,
	    newERE : [ ],
	    newStatements : [ ]
	}
    } else {
	// consider a candidate
	var candidate = uniformDraw(cluster.candidates)
	    
	// choose a threshold for it
	var tau = gamma( thresholdParameters )

	// choose whether it is a member
	var isMember = flip()
	// display(["considering candidate", candidate, isMember, tau, candidateProximity(cluster, candidate)])

    

    	// The candidate needs to be on the right side of threshold tau
    	if (isMember) {
    	    condition( candidateProximity(cluster, candidate) > tau)

    	    return clusterAddCandidate(cluster, candidate)
	    
    	} else {
    	    condition( candidateProximity( cluster, candidate) < tau)
	    
    	    return clusterDropCandidate(cluster, candidate)
    	}
    }
}



//-------------------------------
// Inference
//-----

// Recursive sample for the particle filter
var recursivelyExtendCluster = function(cluster, thresholdParameters) {

    if (clusterNoMoreQueryConstraints(cluster) && clusterNoMoreCandidateStatements(cluster)) {
	// nothing more to add to this cluster, end recursion
	// display(["done with", cluster])
	return cluster;
    } else {
	// try to do one step of additions to the cluster
	// this returns a structure with entries
	// newCluster: updated cluster
	// newERE: EREs added to the frontier of the cluster
	// newStatements: statements added to the cluster
	var result1 = extendClusterOnce(cluster, thresholdParameters)
	
	// take coreference into account
	var result2 = corefDo(result1.newCluster, result1.newERE)

	// candidates: old candidates plus statements adjacent to the new EREs,
	// minus statements already considered
	var updatedCandidates = mySetDiff(
	    aidaAddStatementsFromEre(result2.newCluster.candidates, result2.newEre),
	    result2.newCluster.considered)

	var cluster2 = clusterObjectUpdateCandidates(result2.newCluster, updatedCandidates)
	
	// check logical consistency constraints for the new statements
	map(
	    function(candidate) {
		logicalConsistency(cluster2, candidate)
	    }, result1.newStatements
	)

	// any candidate statements that are duplicates of the statement we just added?
	// HIER: not implemented yet
	
	// and repeat
	return recursivelyExtendCluster( cluster2, thresholdParameters)
    }
}
    


var model = function() {
    // cluster seed:
    // draw from entry points
    var clusterSeed =  uniformDraw(aidaData["entrypoints"])
    // display(["cluster seed", clusterSeed])

    // coreference resolution: we use a mem'ed function
    // to remember which EREs belong to which coref group
    var corefGroupMember = mem(function(corefGroup, ere, confidence) {
	return flip(confidence)
    })    

    // from this cluster seed, make a cluster of our internal format
    var clusterMyFormat = clusterObjectInit(clusterSeed, corefGroupMember)
    
    // compute coref membership for the initial EREs
    var clusterEreCoref = corefDo(clusterMyFormat, clusterMyFormat.cluster.ere)

    // and compute candidates
    var clusterInitial = clusterObjectUpdateCandidates(clusterEreCoref.newCluster,
						 mySetDiff(aidaAddStatementsFromEre([ ], clusterEreCoref.newCluster.cluster.ere),
							   clusterEreCoref.newCluster.cluster.statements))


    var clusterFinal = recursivelyExtendCluster(clusterInitial, aidaData.parameters)
    
    return {
	statements : sort(clusterFinal.cluster.statements.concat(
	    clusterFinal.cluster.ere).concat(
		clusterFinal.cluster.corefStatements).concat(
		    clusterFinal.cluster.corefGroups)),
	failedQueries : clusterFinal.query.failed
    }
}
			  

var Experiment = function () {
    return Infer( { method : "SMC", particles : 1 }, model)
}

//-------------------------------
var result = Experiment()

json.write('aidaresult.json', result);

"AIDA baseline done. Results written to aidaresult.json."
